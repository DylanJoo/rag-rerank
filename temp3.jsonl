{"qid": "1", "topic": "Are there any research papers on methods to compress large-scale language models using task-agnostic knowledge distillation techniques?", "contexts": ["We propose a learning based data augmentation technique for knowledge distillation, called CILDA. To the best of our knowledge, this is the first time that intermediate layer representations of the main task are used in improving the quality of augmented samples.", "A case study of knowledge distillation for compressing multilingual neural machine translation models. We propose a method for achieving this goal. We propose a method for achieving this goal.", "We propose a hierarchical relational knowledge distillation method to compress large-scale language models using task-agnostic knowledge distillation techniques.", "We study knowledge distillation with a focus on multilingual Named Entity Recognition (NER) and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works.", "We propose a novel twostage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT). To avoid text generation in discrete space, we introduce a Plug & Play Embedding Guessing method to craft pseudo embeddings from the teacher's hidden knowledge. Meanwhile, with a self-supervised module to quantify the student's ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner.", "We propose MixKD, a data-agnostic distillation framework that leverages mixup, a simple yet efficient data augmentation approach, to endow the resulting model with stronger generalization ability.", "We propose a method for learning language-agnostic sentence embeddings with lightweight models. We also propose a method for learning language-agnostic sentence embeddings with knowledge distillation.", "Is there a research paper on task-agnostic distillation? - a novel task-agnostic distillation approach equipped with iterative pruning.", "We propose a multi-teacher knowledge distillation framework named MT-BERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs.", "A systematic study of task-specific knowledge distillation techniques for natural language generation with Pseudo-Target Training. We propose Joint-Teaching method for enhancing the model design."], "context_type": "google/flan-t5-base_10", "docids": ["248227350", "258212842", "239015981", "218502458", "222290473", "226226888", "256900817", "257038997", "235294276", "258461336"]}
