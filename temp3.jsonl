{"qid": "1", "topic": "Are there any research papers on methods to compress large-scale language models using task-agnostic knowledge distillation techniques?", "contexts": ["We propose a learning based data augmentation technique for knowledge distillation, called CILDA. To the best of our knowledge, this is the first time that intermediate layer representations of the main task are used in improving the quality of augmented samples.", "A case study of knowledge distillation for compressing multilingual neural machine translation models. We propose a method for achieving this goal. We propose a method for achieving this goal.", "We study knowledge distillation with a focus on multilingual Named Entity Recognition (NER) and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works.", "We propose MixKD, a data-agnostic distillation framework that leverages mixup, a simple yet efficient data augmentation approach, to endow the resulting model with stronger generalization ability.", "Is there a research paper on task-agnostic distillation? - a novel task-agnostic distillation approach equipped with iterative pruning.", "We propose a multi-teacher knowledge distillation framework named MT-BERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs.", "We propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: (i) PKD-Last: learning from the last k layers; and (ii) PKD-Skip: learning from every k layers. These two patient distillation schemes enable the exploitation of rich information in the teacher's hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multilayer distillation process. Empirically, this translates into improved results on multiple NLP tasks with significant gain in training efficiency, without sacrificing model accuracy.", "The CoDIR framework is a principled knowledge distillation framework that allows the student to distill knowledge through intermediate layers of the teacher. It is a novel approach to compress large-scale language models.", "We compare accuracy vs. model size tradeoffs across six BERT architecture sizes and eight GLUE tasks. We find that quantization and distillation consistently provide greater benefit than pruning.", "We present a new knowledge distillation objective for language representation learning that transfers the contextual knowledge via two types of relationships across representations: Word Relation and Layer Transforming Relation."], "context_type": "google/flan-t5-base", "docids": ["248227350", "258212842", "218502458", "226226888", "257038997", "235294276", "201670719", "221995575", "247741658", "237563200"]}
